{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Author: Sebastien Boisvert <sebhtml@protonmail.com>\n# Git repository: https://github.com/sebhtml/Arc-Prize-2024-sebhtml\n\n# - TODO use CUDA (NVIDIA P100) on Kaggle\n# - TODO model should take in input (input_state, current_state)\n# The model predicts (action_cell, action_value) (write <action_value> to cell <action_cell>)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-07-18T17:58:05.299013Z\",\"iopub.execute_input\":\"2024-07-18T17:58:05.299454Z\",\"iopub.status.idle\":\"2024-07-18T17:58:05.306369Z\",\"shell.execute_reply.started\":\"2024-07-18T17:58:05.299420Z\",\"shell.execute_reply\":\"2024-07-18T17:58:05.305066Z\"}}\n# https://www.kaggle.com/code/sebastien/arc-prize-2024-sebhtml/edit\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nimport torch\nfrom torch import nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import functional as F\nimport os\nimport itertools\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-07-18T17:58:05.308582Z\",\"iopub.execute_input\":\"2024-07-18T17:58:05.309436Z\",\"iopub.status.idle\":\"2024-07-18T17:58:05.324028Z\",\"shell.execute_reply.started\":\"2024-07-18T17:58:05.309403Z\",\"shell.execute_reply\":\"2024-07-18T17:58:05.322739Z\"}}\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#\n# /kaggle/input/arc-prize-2024/arc-agi_training_challenges.json\n# /kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\n#\n# /kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\n# /kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\n#\n# /kaggle/input/arc-prize-2024/arc-agi_test_challenges.json\n#\n# /kaggle/input/arc-prize-2024/sample_submission.json\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-07-18T17:58:05.325644Z\",\"iopub.execute_input\":\"2024-07-18T17:58:05.326104Z\",\"iopub.status.idle\":\"2024-07-18T17:58:05.336750Z\",\"shell.execute_reply.started\":\"2024-07-18T17:58:05.326060Z\",\"shell.execute_reply\":\"2024-07-18T17:58:05.335390Z\"}}\nvision_width = 7\nvision_height = 7\noutput_width = 7\noutput_height = 7\nnum_classes = 10\nd_model = 768\nnum_heads = 12\ndropout = 0.5\nnum_layers = 6\nbatch_size = 512\nshuffle = True\nlr = 0.001\nnum_epochs = 100\nnum_layers = 12\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-07-18T17:58:05.339070Z\",\"iopub.execute_input\":\"2024-07-18T17:58:05.339468Z\",\"iopub.status.idle\":\"2024-07-18T17:58:05.349542Z\",\"shell.execute_reply.started\":\"2024-07-18T17:58:05.339437Z\",\"shell.execute_reply\":\"2024-07-18T17:58:05.348400Z\"}}\ndef generate_action_examples(puzzle_example):\n    (example_input, example_output) = puzzle_example\n    action_examples = []\n    current_state = example_input\n    for i in range(len(example_input)):\n        input_pixel = example_input[i]\n        output_pixel = example_output[i]\n        if input_pixel != output_pixel:\n            action_cell = i\n            action_value = output_pixel\n            example = (current_state, (action_cell, action_value))\n            action_examples.append(example)\n            # Update current_state\n            current_state = current_state.copy()\n            current_state[action_cell] = action_value\n    return action_examples\n\ndef load_puzzle_train_examples(file_path, problem):\n    f = open(file_path)\n    data = json.load(f)\n    problem_data = data[problem]\n    puzzle_examples = problem_data[\"train\"]\n    puzzle_train_examples = []\n    for puzzle_example in puzzle_examples:\n        example_input = puzzle_example[\"input\"]\n        example_output = puzzle_example[\"output\"]\n        example_input = list(itertools.chain(*example_input))\n        example_output = list(itertools.chain(*example_output))\n        example = (example_input, example_output)\n        puzzle_train_examples.append(example)\n    return puzzle_train_examples\n\ndef generate_train_action_examples(puzzle_examples):\n    train_examples = []\n    for puzzle_example in puzzle_examples:\n        for action_example in generate_action_examples(puzzle_example):\n            train_examples.append(action_example)\n    return train_examples\n\ndef make_example_input_tensor(puzzle_example_current_state):\n    item_input = torch.tensor(puzzle_example_current_state)\n    item_input = F.one_hot(item_input, num_classes=num_classes).float()\n    return item_input\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-07-18T17:58:05.396980Z\",\"iopub.execute_input\":\"2024-07-18T17:58:05.397548Z\",\"iopub.status.idle\":\"2024-07-18T17:58:05.406723Z\",\"shell.execute_reply.started\":\"2024-07-18T17:58:05.397499Z\",\"shell.execute_reply\":\"2024-07-18T17:58:05.405477Z\"}}\nclass MyDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        \n        item_input = make_example_input_tensor(example[0])\n        item_output = example[1]\n\n        action_cell = item_output[0]\n        action_cell = torch.tensor(action_cell)\n        action_cell = F.one_hot(action_cell, num_classes=output_width * output_height).float()\n\n        action_value = item_output[1]\n        action_value = torch.tensor(action_value)\n        action_value = F.one_hot(action_value, num_classes=num_classes).float()\n\n        item_output = (action_cell, action_value)\n        item = (item_input, item_output)\n        return item\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-07-18T17:58:05.409064Z\",\"iopub.execute_input\":\"2024-07-18T17:58:05.409463Z\",\"iopub.status.idle\":\"2024-07-18T17:58:05.436922Z\",\"shell.execute_reply.started\":\"2024-07-18T17:58:05.409420Z\",\"shell.execute_reply\":\"2024-07-18T17:58:05.435808Z\"}}\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    def __init__(self, n_embd, dropout):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.GELU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass NonCausalSelfAttentionTransformerBlock(nn.Module):\n    def __init__(self, d_model, num_heads, dropout):\n        super(NonCausalSelfAttentionTransformerBlock, self).__init__()\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n        self.ffwd = FeedForward(d_model, dropout)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n\n    def forward(self, src):\n        src_ln = self.ln1(src)\n        attn_output, attn_output_weights = self.multihead_attn(src_ln, src_ln, src_ln)\n        #print(\"attn_output\")\n        #print(attn_output)\n        src_and_sa = self.ln2(src + attn_output)\n        src_and_sa_and_ffwd = src_and_sa + self.ffwd(src_and_sa)\n        return src_and_sa_and_ffwd\n        \nclass DecoderOnlyTransformerModel(nn.Module):\n    def __init__(self, num_classes, d_model, dropout, num_heads):\n        super(DecoderOnlyTransformerModel, self).__init__()\n        self.embed = nn.Linear(in_features=num_classes, out_features=d_model, bias=False)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.blocks = nn.Sequential(\n            NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            #NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            #NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            #NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            #NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            #NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            #NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            #NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            #NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n            #NonCausalSelfAttentionTransformerBlock(d_model, num_heads, dropout),\n        )\n        self.ln = nn.LayerNorm(normalized_shape=d_model)\n\n        self.action_cell_lin = nn.Linear(\n            in_features=output_width * output_height * d_model,\n            out_features=output_width * output_height)\n        self.action_value_lin = nn.Linear(\n            in_features=output_width * output_height * d_model,\n            out_features=num_classes)\n        self.action_cell_soft = nn.Softmax(dim=-1)\n        self.action_value_soft = nn.Softmax(dim=-1)\n\n    def forward(self, src):\n        embed = self.embed(src)\n        embed_drop = self.dropout_1(embed)\n        transformed = self.blocks(embed_drop)\n        transformed_ln = self.ln(transformed)\n        size = transformed_ln.size()\n        reshaped = transformed_ln.view([size[0], size[1] * size[2]])\n        action_cell = self.action_cell_lin(reshaped)\n        action_value = self.action_value_lin(reshaped)\n        return (action_cell, action_value)\n        action_cell = self.action_cell_soft(self.action_cell_lin(reshaped))\n        action_value = self.action_value_soft(self.action_value_lin(reshaped))\n        return (action_cell, action_value)\n\ndef get_grad_norm(model):\n    \"\"\"\n    Calculates and prints the total L2 norm of the gradients of all model parameters.\n\n    Args:\n    model (torch.nn.Module): The PyTorch model whose gradients you want to monitor.\n    \"\"\"\n    total_norm = 0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2).item()**2  # Square for L2 norm\n            total_norm += param_norm\n        total_norm = total_norm**0.5  # Take the square root for L2 norm\n\n    return total_norm\n\ndef print_predicted_actions():\n    for data in train_loader:\n        (inputs, targets) = data\n        print(inputs.size())\n        outputs = model(inputs) \n        for idx in range(len(inputs)):\n            initial_state = inputs[idx].argmax(dim=-1)\n            target_action_cell = targets[0][idx].argmax(dim=-1).item()\n            target_action_value = targets[1][idx].argmax(dim=-1).item()\n            output_action_cell = outputs[0][idx].argmax(dim=-1).item()\n            output_action_value = outputs[1][idx].argmax(dim=-1).item()\n            print(\"Example: \" + str(idx))\n            print(\"initial_state: \" + str(initial_state))\n            print(\"target_action_cell: \" + str(target_action_cell))\n            print(\"target_action_value: \" + str(target_action_value))\n            print(\"output_action_cell: \" + str(output_action_cell))\n            print(\"output_action_value: \" + str(output_action_value))\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-07-18T17:58:05.438134Z\",\"iopub.execute_input\":\"2024-07-18T17:58:05.438488Z\",\"iopub.status.idle\":\"2024-07-18T17:58:05.684885Z\",\"shell.execute_reply.started\":\"2024-07-18T17:58:05.438459Z\",\"shell.execute_reply\":\"2024-07-18T17:58:05.683364Z\"}}\nmodel = DecoderOnlyTransformerModel(num_classes, d_model, dropout, num_heads)\n\ncriterion = nn.CrossEntropyLoss()\nmodel_total_params = sum(p.numel() for p in model.parameters())\nprint(\"Model parameters: \" + str(model_total_params))\noptimizer = AdamW(model.parameters(), lr=lr)\n\npuzzle_train_examples = load_puzzle_train_examples(\"/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json\", \"3aa6fb7a\")\ntrain_action_examples = generate_train_action_examples(puzzle_train_examples)\n\ndataset = MyDataset(train_action_examples)\ntrain_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\nprint(\"Train Examples\")\nprint(len(train_action_examples))\nfor (idx, example) in enumerate(train_action_examples):\n    print(\"Example: \" + str(idx))\n    print(\"initial_state: \" + str(example[0]))\n    print(\"output_action_cell: \" + str(example[1][0]))\n    print(\"output_action_value: \" + str(example[1][1]))\n\nglobal_step = 0\nfor epoch in range(num_epochs):\n    for data in train_loader:\n        optimizer.zero_grad()\n        (inputs, targets) = data\n        outputs = model(inputs)\n        cell_loss = criterion(outputs[0], targets[0])\n        pixel_loss = criterion(outputs[1], targets[1])\n        loss = cell_loss + pixel_loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        grad_l2_norm = get_grad_norm(model)\n        print(f\"Epoch: {epoch + 1} / {num_epochs}  global_step: {global_step + 1}  loss: {loss}  grad_l2_norm: {grad_l2_norm:.4f}\")\n        global_step += 1\n        \nprint(\"[after training] print_predicted_actions\")\nprint_predicted_actions()\n\ndef solve_puzzle_example_auto_regressive(input_state, current_state):\n    print(\"AUTO-REGRESSIVE wannabe AGI megabot\")\n    print(\"input_state\")\n    print(input_state)\n    print(\"current_state on entry\")\n    print(current_state)\n    # TODO improve stopping criterion\n    for i in range(10):\n        inputs = make_example_input_tensor(current_state).unsqueeze(0)\n        outputs = model(inputs)\n        (action_cell, action_value) = (outputs[0][0].argmax(dim=-1).item(), outputs[1][0].argmax(dim=-1).item())\n        current_state = current_state.copy()\n        current_state[action_cell] = action_value\n        print(\"current_state after motor action\")\n        print(current_state)\n    return current_state\n\npuzzle_example_input = puzzle_train_examples[0][0]\npuzzle_example_output = puzzle_train_examples[0][1]\nsolve_puzzle_example_auto_regressive(puzzle_example_input, puzzle_example_input)\nprint(\"Expected output\")\nprint(puzzle_example_output)","metadata":{"_uuid":"710405b9-3491-43f0-8319-df8fc01864d3","_cell_guid":"13c91187-f4ef-44c5-9633-d13de9d820ae","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-18T20:00:41.529210Z","iopub.execute_input":"2024-07-18T20:00:41.529647Z","iopub.status.idle":"2024-07-18T20:01:25.156463Z","shell.execute_reply.started":"2024-07-18T20:00:41.529607Z","shell.execute_reply":"2024-07-18T20:01:25.155037Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Model parameters: 23493179\nTrain Examples\n5\nExample: 0\ninitial_state: [0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\noutput_action_cell: 9\noutput_action_value: 1\nExample: 1\ninitial_state: [0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\noutput_action_cell: 32\noutput_action_value: 1\nExample: 2\ninitial_state: [0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 8, 0, 0]\noutput_action_cell: 11\noutput_action_value: 1\nExample: 3\ninitial_state: [0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 8, 0, 0]\noutput_action_cell: 17\noutput_action_value: 1\nExample: 4\ninitial_state: [0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 8, 0, 0]\noutput_action_cell: 38\noutput_action_value: 1\nEpoch: 1 / 100  global_step: 1  loss: 7.0233869552612305  grad_l2_norm: 1.1592\nEpoch: 2 / 100  global_step: 2  loss: 1.5616384744644165  grad_l2_norm: 1.0435\nEpoch: 3 / 100  global_step: 3  loss: 12.373006820678711  grad_l2_norm: 1.0434\nEpoch: 4 / 100  global_step: 4  loss: 69.93294525146484  grad_l2_norm: 1.0960\nEpoch: 5 / 100  global_step: 5  loss: 12.156634330749512  grad_l2_norm: 1.0404\nEpoch: 6 / 100  global_step: 6  loss: 25.516128540039062  grad_l2_norm: 1.0419\nEpoch: 7 / 100  global_step: 7  loss: 91.98980712890625  grad_l2_norm: 1.0467\nEpoch: 8 / 100  global_step: 8  loss: 16.285858154296875  grad_l2_norm: 1.0431\nEpoch: 9 / 100  global_step: 9  loss: 9.048532485961914  grad_l2_norm: 1.0437\nEpoch: 10 / 100  global_step: 10  loss: 3.5859458446502686  grad_l2_norm: 1.0437\nEpoch: 11 / 100  global_step: 11  loss: 6.578495979309082  grad_l2_norm: 1.0435\nEpoch: 12 / 100  global_step: 12  loss: 8.149258613586426  grad_l2_norm: 1.0436\nEpoch: 13 / 100  global_step: 13  loss: 4.67730712890625  grad_l2_norm: 1.0440\nEpoch: 14 / 100  global_step: 14  loss: 1.6511163711547852  grad_l2_norm: 1.0440\nEpoch: 15 / 100  global_step: 15  loss: 4.522176742553711  grad_l2_norm: 1.0440\nEpoch: 16 / 100  global_step: 16  loss: 6.3587446212768555  grad_l2_norm: 1.0440\nEpoch: 17 / 100  global_step: 17  loss: 4.420779228210449  grad_l2_norm: 1.0441\nEpoch: 18 / 100  global_step: 18  loss: 2.4436392784118652  grad_l2_norm: 1.0441\nEpoch: 19 / 100  global_step: 19  loss: 1.4369443655014038  grad_l2_norm: 1.0440\nEpoch: 20 / 100  global_step: 20  loss: 1.6338310241699219  grad_l2_norm: 1.0441\nEpoch: 21 / 100  global_step: 21  loss: 1.6015825271606445  grad_l2_norm: 1.0441\nEpoch: 22 / 100  global_step: 22  loss: 1.5479129552841187  grad_l2_norm: 1.0442\nEpoch: 23 / 100  global_step: 23  loss: 0.596199095249176  grad_l2_norm: 1.0441\nEpoch: 24 / 100  global_step: 24  loss: 0.26550132036209106  grad_l2_norm: 1.0442\nEpoch: 25 / 100  global_step: 25  loss: 0.5174247026443481  grad_l2_norm: 1.0441\nEpoch: 26 / 100  global_step: 26  loss: 0.583209753036499  grad_l2_norm: 1.0441\nEpoch: 27 / 100  global_step: 27  loss: 0.49215346574783325  grad_l2_norm: 1.0441\nEpoch: 28 / 100  global_step: 28  loss: 0.07143419235944748  grad_l2_norm: 1.0441\nEpoch: 29 / 100  global_step: 29  loss: 0.002138429554179311  grad_l2_norm: 1.0039\nEpoch: 30 / 100  global_step: 30  loss: 0.47638773918151855  grad_l2_norm: 1.0441\nEpoch: 31 / 100  global_step: 31  loss: 0.22981417179107666  grad_l2_norm: 1.0441\nEpoch: 32 / 100  global_step: 32  loss: 0.2884576618671417  grad_l2_norm: 1.0442\nEpoch: 33 / 100  global_step: 33  loss: 0.05950043722987175  grad_l2_norm: 1.0442\nEpoch: 34 / 100  global_step: 34  loss: 0.0008314281003549695  grad_l2_norm: 1.0020\nEpoch: 35 / 100  global_step: 35  loss: 0.004731330089271069  grad_l2_norm: 1.0269\nEpoch: 36 / 100  global_step: 36  loss: 0.1788555532693863  grad_l2_norm: 1.0442\nEpoch: 37 / 100  global_step: 37  loss: 0.02198251336812973  grad_l2_norm: 1.0442\nEpoch: 38 / 100  global_step: 38  loss: 0.0009525712812319398  grad_l2_norm: 1.0022\nEpoch: 39 / 100  global_step: 39  loss: 0.024333393201231956  grad_l2_norm: 1.0442\nEpoch: 40 / 100  global_step: 40  loss: 0.0014613515231758356  grad_l2_norm: 1.0088\nEpoch: 41 / 100  global_step: 41  loss: 0.0028242964763194323  grad_l2_norm: 1.0236\nEpoch: 42 / 100  global_step: 42  loss: 0.005511555355042219  grad_l2_norm: 1.0442\nEpoch: 43 / 100  global_step: 43  loss: 0.0006623361259698868  grad_l2_norm: 1.0018\nEpoch: 44 / 100  global_step: 44  loss: 8.455818897346035e-05  grad_l2_norm: 1.0000\nEpoch: 45 / 100  global_step: 45  loss: 3.177951293764636e-05  grad_l2_norm: 1.0000\nEpoch: 46 / 100  global_step: 46  loss: 8.951656491262838e-05  grad_l2_norm: 1.0000\nEpoch: 47 / 100  global_step: 47  loss: 3.711952012963593e-05  grad_l2_norm: 1.0000\nEpoch: 48 / 100  global_step: 48  loss: 0.00014859295333735645  grad_l2_norm: 1.0001\nEpoch: 49 / 100  global_step: 49  loss: 9.423606388736516e-05  grad_l2_norm: 1.0000\nEpoch: 50 / 100  global_step: 50  loss: 0.0003968224918935448  grad_l2_norm: 1.0007\nEpoch: 51 / 100  global_step: 51  loss: 0.001153608551248908  grad_l2_norm: 1.0031\nEpoch: 52 / 100  global_step: 52  loss: 0.00023858505301177502  grad_l2_norm: 1.0001\nEpoch: 53 / 100  global_step: 53  loss: 0.00040653295582160354  grad_l2_norm: 1.0003\nEpoch: 54 / 100  global_step: 54  loss: 0.00030628417152911425  grad_l2_norm: 1.0004\nEpoch: 55 / 100  global_step: 55  loss: 4.091139999218285e-05  grad_l2_norm: 1.0000\nEpoch: 56 / 100  global_step: 56  loss: 9.427646000403911e-05  grad_l2_norm: 1.0000\nEpoch: 57 / 100  global_step: 57  loss: 2.6748928576125763e-05  grad_l2_norm: 1.0000\nEpoch: 58 / 100  global_step: 58  loss: 1.5783047274453565e-05  grad_l2_norm: 1.0000\nEpoch: 59 / 100  global_step: 59  loss: 1.4471756003331393e-05  grad_l2_norm: 1.0000\nEpoch: 60 / 100  global_step: 60  loss: 3.4187425626441836e-05  grad_l2_norm: 1.0000\nEpoch: 61 / 100  global_step: 61  loss: 3.8169171602930874e-05  grad_l2_norm: 1.0000\nEpoch: 62 / 100  global_step: 62  loss: 1.8214912415714934e-05  grad_l2_norm: 1.0000\nEpoch: 63 / 100  global_step: 63  loss: 0.00015091439126990736  grad_l2_norm: 1.0001\nEpoch: 64 / 100  global_step: 64  loss: 3.843031299766153e-05  grad_l2_norm: 1.0000\nEpoch: 65 / 100  global_step: 65  loss: 2.6415738830110058e-05  grad_l2_norm: 1.0000\nEpoch: 66 / 100  global_step: 66  loss: 2.6463600079296157e-05  grad_l2_norm: 1.0000\nEpoch: 67 / 100  global_step: 67  loss: 1.6641093679936603e-05  grad_l2_norm: 1.0000\nEpoch: 68 / 100  global_step: 68  loss: 3.366264718351886e-05  grad_l2_norm: 1.0000\nEpoch: 69 / 100  global_step: 69  loss: 1.0895610103034414e-05  grad_l2_norm: 1.0000\nEpoch: 70 / 100  global_step: 70  loss: 5.531268016056856e-06  grad_l2_norm: 1.0000\nEpoch: 71 / 100  global_step: 71  loss: 7.486250069632661e-06  grad_l2_norm: 1.0000\nEpoch: 72 / 100  global_step: 72  loss: 7.104818450898165e-06  grad_l2_norm: 1.0000\nEpoch: 73 / 100  global_step: 73  loss: 1.814306597225368e-05  grad_l2_norm: 1.0000\nEpoch: 74 / 100  global_step: 74  loss: 1.4042510883882642e-05  grad_l2_norm: 1.0000\nEpoch: 75 / 100  global_step: 75  loss: 5.030587090004701e-06  grad_l2_norm: 1.0000\nEpoch: 76 / 100  global_step: 76  loss: 3.719311962413485e-06  grad_l2_norm: 1.0000\nEpoch: 77 / 100  global_step: 77  loss: 7.772339813527651e-06  grad_l2_norm: 1.0000\nEpoch: 78 / 100  global_step: 78  loss: 5.674339718098054e-06  grad_l2_norm: 1.0000\nEpoch: 79 / 100  global_step: 79  loss: 4.148452717345208e-06  grad_l2_norm: 1.0000\nEpoch: 80 / 100  global_step: 80  loss: 1.58304137585219e-05  grad_l2_norm: 1.0000\nEpoch: 81 / 100  global_step: 81  loss: 4.0531145373279287e-07  grad_l2_norm: 1.0000\nEpoch: 82 / 100  global_step: 82  loss: 1.0013571909439634e-06  grad_l2_norm: 1.0000\nEpoch: 83 / 100  global_step: 83  loss: 3.647782477855799e-06  grad_l2_norm: 1.0000\nEpoch: 84 / 100  global_step: 84  loss: 1.3637266420118976e-05  grad_l2_norm: 1.0000\nEpoch: 85 / 100  global_step: 85  loss: 1.8358168745180592e-06  grad_l2_norm: 1.0000\nEpoch: 86 / 100  global_step: 86  loss: 3.4332165341766085e-06  grad_l2_norm: 1.0000\nEpoch: 87 / 100  global_step: 87  loss: 2.6702821287472034e-06  grad_l2_norm: 1.0000\nEpoch: 88 / 100  global_step: 88  loss: 7.438589818775654e-06  grad_l2_norm: 1.0000\nEpoch: 89 / 100  global_step: 89  loss: 2.2649708171229577e-06  grad_l2_norm: 1.0000\nEpoch: 90 / 100  global_step: 90  loss: 3.838514203380328e-06  grad_l2_norm: 1.0000\nEpoch: 91 / 100  global_step: 91  loss: 2.0265529201424215e-06  grad_l2_norm: 1.0000\nEpoch: 92 / 100  global_step: 92  loss: 4.434551556187216e-06  grad_l2_norm: 1.0000\nEpoch: 93 / 100  global_step: 93  loss: 1.0943282177322544e-05  grad_l2_norm: 1.0000\nEpoch: 94 / 100  global_step: 94  loss: 3.766998361243168e-06  grad_l2_norm: 1.0000\nEpoch: 95 / 100  global_step: 95  loss: 2.074237727356376e-06  grad_l2_norm: 1.0000\nEpoch: 96 / 100  global_step: 96  loss: 3.480898158159107e-06  grad_l2_norm: 1.0000\nEpoch: 97 / 100  global_step: 97  loss: 4.219987204123754e-06  grad_l2_norm: 1.0000\nEpoch: 98 / 100  global_step: 98  loss: 3.1947924981068354e-06  grad_l2_norm: 1.0000\nEpoch: 99 / 100  global_step: 99  loss: 2.0503957784967497e-06  grad_l2_norm: 1.0000\nEpoch: 100 / 100  global_step: 100  loss: 6.222630872798618e-06  grad_l2_norm: 1.0000\n[after training] print_predicted_actions\ntorch.Size([5, 49, 10])\nExample: 0\ninitial_state: tensor([0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8,\n        8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 8, 0,\n        0])\ntarget_action_cell: 17\ntarget_action_value: 1\noutput_action_cell: 17\noutput_action_value: 1\nExample: 1\ninitial_state: tensor([0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8,\n        8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 8, 0,\n        0])\ntarget_action_cell: 38\ntarget_action_value: 1\noutput_action_cell: 38\noutput_action_value: 1\nExample: 2\ninitial_state: tensor([0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0,\n        0, 8, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0])\ntarget_action_cell: 32\ntarget_action_value: 1\noutput_action_cell: 32\noutput_action_value: 1\nExample: 3\ninitial_state: tensor([0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8,\n        8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 8, 0,\n        0])\ntarget_action_cell: 11\ntarget_action_value: 1\noutput_action_cell: 11\noutput_action_value: 1\nExample: 4\ninitial_state: tensor([0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0,\n        0, 8, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0])\ntarget_action_cell: 9\ntarget_action_value: 1\noutput_action_cell: 9\noutput_action_value: 1\nAUTO-REGRESSIVE wannabe AGI megabot\ninput_state\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state on entry\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state after motor action\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state after motor action\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state after motor action\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state after motor action\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state after motor action\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state after motor action\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state after motor action\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state after motor action\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state after motor action\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ncurrent_state after motor action\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nExpected output\n[0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}]}]}